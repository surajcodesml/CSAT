# CSAT Hyperparameter Experiments - Quick Start Guide

## Current Status

**Experiment 1 (Baseline)** is currently running with 20 epochs (~20-30 minutes).

## Option 1: Run All Experiments Automatically (Recommended)

If you want to start fresh and run all 6 experiments sequentially in the background:

```bash
# Stop current experiment 1 if desired (Ctrl+C in that terminal)

# Make scripts executable
chmod +x run_all_experiments.sh

# Run in background with nohup
nohup ./run_all_experiments.sh > logs/full_run.log 2>&1 &

# Monitor progress
tail -f logs/full_run.log

# Or check specific experiment
tail -f logs/exp1_baseline_output.txt
```

## Option 2: Run Remaining Experiments After Exp 1 Finishes

If you want to let experiment 1 finish, then automatically run experiments 2-6:

```bash
# Wait for experiment 1 to complete, then:
chmod +x run_experiments_2_to_6.sh
nohup ./run_experiments_2_to_6.sh > logs/remaining_experiments.log 2>&1 &

# Monitor
tail -f logs/remaining_experiments.log
```

## Monitoring Progress

### Check Overall Progress
```bash
# Watch summary of all running processes
tail -f logs/full_run.log

# Check GPU usage
watch -n 2 rocm-smi  # or nvidia-smi for NVIDIA GPUs
```

### Check Specific Experiment
```bash
# Watch a specific experiment output
tail -f logs/exp1_baseline_output.txt

# Check JSON metrics (updates after each epoch)
cat logs/exp1_baseline_*.json | jq '.epochs[-1]'  # requires jq
# or just:
cat logs/exp1_baseline_*.json
```

### Check Which Experiments Have Completed
```bash
# List completed model checkpoints
ls -lh runs/exp*/detection_best.pth

# List JSON logs (one per experiment)
ls -lh logs/*.json
```

## Expected Timeline

- **Experiments 1-5**: 20 epochs each × ~1-2 min/epoch = ~20-40 min each
- **Experiment 6**: 50 epochs × ~1-2 min/epoch = ~50-100 min
- **Total**: ~3-5 hours for all 6 experiments

## Compare Results

After experiments complete:

```bash
# Install tabulate if needed
pip install tabulate

# Compare all experiments
python compare_experiments.py

# This will show:
# - Configuration comparison table
# - Performance metrics table  
# - Best performing configuration
# - Recommendations
```

## Manual Experiment Runs

To run experiments individually:

```bash
# Activate environment
conda activate oct

# Run specific experiment
python train.py --config configs/exp1_baseline.yaml
python train.py --config configs/exp2_lr_tuning.yaml
# etc...
```

## Troubleshooting

### Out of Memory
If you get CUDA/GPU OOM errors, reduce batch size:
```bash
python train.py --config configs/exp5_batch_training.yaml  # This uses batch=32
```

### Check if Experiments are Running
```bash
ps aux | grep "train.py"
```

### Kill All Running Experiments
```bash
pkill -f "train.py"
```

## Output Files

After running experiments, you'll have:

```
CSAT/
├── logs/
│   ├── exp1_baseline_20251217_*.json      # Metrics per epoch
│   ├── exp1_baseline_output.txt           # Full training log
│   ├── exp2_lr_tuning_20251217_*.json
│   ├── exp2_lr_tuning_output.txt
│   └── ... (for all experiments)
│   └── comparison_report.txt              # Generated by compare_experiments.py
├── runs/
│   ├── exp1_baseline/detection_best.pth   # Best model checkpoint
│   ├── exp1_baseline/detection_last.pth   # Last epoch checkpoint
│   ├── exp2_lr_tuning/...
│   └── ... (for all experiments)
└── configs/
    ├── exp1_baseline.yaml
    ├── exp2_lr_tuning.yaml
    └── ... (6 config files)
```

## What Each Experiment Tests

1. **exp1_baseline**: Current config (LR=0.0002, batch=128, queries=100)
2. **exp2_lr_tuning**: Lower LR=0.00005, gentler scheduler
3. **exp3_loss_balanced**: Balanced loss weights (CE:2, BBox:5, GIoU:2)
4. **exp4_architecture**: Fewer queries=50, higher dropout=0.15
5. **exp5_batch_training**: Smaller batch=32 for gradient diversity
6. **exp6_optimized**: Combined best settings (50 epochs)

## Next Steps After Experiments

1. Run `python compare_experiments.py` to see results
2. Identify best configuration
3. Update main `config.yaml` with best parameters
4. Run longer training (100+ epochs) with best config
5. Validate on test set
