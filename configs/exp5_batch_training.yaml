# Experiment 5: Batch Size and Training Duration
# Purpose: Test smaller batches for better gradient diversity
# Hypothesis: Large batch (128) reduces stochasticity needed for exploration

# Paths
paths:
  root: './'
  dataroot: './pickle'
  pretrain_weights: 'model/best_pretrainer.pth'
  output_dir: 'runs/exp5_batch_training/'
  train_data: 'data/train.txt'
  val_data: 'data/val.txt'

# Model Architecture (same as baseline)
model:
  hidden_dim: 256
  num_encoder_layers: 6
  num_decoder_layers: 6
  nheads: 8
  num_queries: 100
  num_classes: 2
  dropout: 0.1

# Training Parameters
training:
  epochs: 20
  train_batch_size: 32  # REDUCED from 128 for noisier gradients
  val_batch_size: 32    # REDUCED from 64
  learning_rate: 0.0001  # Slightly higher for smaller batch
  weight_decay: 0.0001
  
  # Learning rate scheduler
  lr_scheduler:
    type: 'ReduceLROnPlateau'
    patience: 5
    cooldown: 3
    factor: 0.5
    mode: 'min'

# Data Loading
data:
  r: 3
  space: 1
  num_workers: 2
  cache: 'ram'
  shuffle: true

# Distributed Training
distributed:
  world_size: 1
  backend: 'nccl'
  init_method: 'tcp://127.0.0.1:12426'
  timeout: 5000

# Pretrained Weights
pretrain:
  use_pretrained: true
  freeze_encoder: false

# Resume Training
resume:
  enabled: false
  weights_path: 'runs/outputs/detection_best.pth'

# Weights & Biases
wandb:
  enabled: false
  project: 'scr'
  name: 'exp5_batch_training'

# Logging
logging:
  print_freq: 10
  save_best: true
  save_last: true
  experiment_name: 'exp5_batch_training'
